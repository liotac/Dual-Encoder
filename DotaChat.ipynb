{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 764\n",
      "Python: 3.5.4\n",
      "TensorFlow: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import json, nlp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from contextlib import closing\n",
    "from collections import Counter, Iterable\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from platform import python_version\n",
    "print('Seed:', SEED)\n",
    "print('Python:', python_version())\n",
    "print('TensorFlow:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(ProgressTracker):\n",
    "    '''\n",
    "    TODO\n",
    "    average utt per dial\n",
    "    average word per utt\n",
    "    total utt\n",
    "    total word\n",
    "    unique word\n",
    "    '''\n",
    "    def __init__(self, num_classes, label_name, ignore_negative=False, rate=1000, total=None):\n",
    "        super().__init__(rate, total)\n",
    "        self.num_classes = int(num_classes)\n",
    "        self.label_name = label_name\n",
    "        self.ignore_negative = ignore_negative\n",
    "        self._baseline = 0\n",
    "        self._distribution = Counter()\n",
    "        self._vocabulary = Counter()\n",
    "        self._frequencies = Counter()\n",
    "       \n",
    "    def __repr__(self):\n",
    "        return str(self.rate)\n",
    "   \n",
    "    def __call__(self, sample=None, done=False):\n",
    "        if not sample or done:\n",
    "            super().__call__(True)\n",
    "            print('Dialogues:', self.__len__())\n",
    "            print('Utterances:')\n",
    "            print('Words:')\n",
    "            print('Vocabulary size:', len(self.vocabulary))\n",
    "            print('Most common words:', dict(self.vocabulary.most_common(5)))\n",
    "            print('Baseline accuracy: %.2f%%' % (self.baseline*100))\n",
    "            print('Class distribution:', dict(self.distribution.most_common()))\n",
    "            print('Frequencies distribution:', dict(self.frequencies.most_common(10)))\n",
    "        else:\n",
    "            super().__call__(False)\n",
    "            self._distribution[sample[self.label_name]] += 1\n",
    "            for k, v in sample.items():\n",
    "                if self.ignore_negative and sample[self.label_name] == 0:\n",
    "                    continue\n",
    "                if k != self.label_name:\n",
    "                    for i in self.flatten(v):\n",
    "                        self._vocabulary[i] += 1\n",
    "   \n",
    "    def flatten(self, l):\n",
    "        non_iterable = (str, bytes, int)\n",
    "        if isinstance(l, non_iterable):\n",
    "            yield l\n",
    "        else:\n",
    "            for el in l:\n",
    "                if isinstance(el, Iterable) and not isinstance(el, non_iterable):\n",
    "                    yield from self.flatten(el)\n",
    "                else:\n",
    "                    yield el\n",
    "   \n",
    "    @property\n",
    "    def vocabulary(self):\n",
    "        return self._vocabulary\n",
    "   \n",
    "    @property\n",
    "    def distribution(self):\n",
    "         return self._distribution\n",
    "    \n",
    "    @property\n",
    "    def baseline(self):\n",
    "        if not self._baseline:\n",
    "            self._baseline = sum(v**2 for _,v in self.distribution.items())/self._counter\n",
    "        return float(self._baseline)/float(self._counter)\n",
    "    \n",
    "    @property\n",
    "    def frequencies(self):\n",
    "        if not self._frequencies:\n",
    "            for k, v in self._vocabulary.items():\n",
    "                self._frequencies[v] += 1\n",
    "        return self._frequencies\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self._counter\n",
    "    \n",
    "    def graphs():\n",
    "        # prints graphs of word distribution, freq of freq, class distribution\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordWriter:\n",
    "    '''\n",
    "    A class wraper to write data into tfrecord shards. Note that\n",
    "    currently tfrecord does not support appending. Therefore, the\n",
    "    contextlib.closing() library is used to provide safe closing\n",
    "    of file descriptors which stays open across method calls.\n",
    "    Supports sharding and compression.{}\n",
    "    '''\n",
    "    def __init__(self, path, filename=None, shard_size=None, compression=None):\n",
    "        self.path = path\n",
    "        self.filename = filename if filename else 'shard'\n",
    "        self.filename = self.path + '/' + self.filename\n",
    "        self.shard_size = int(shard_size) if shard_size else 0\n",
    "        self.compression = compression #TODO\n",
    "        self.shards = 0\n",
    "        self.counter = 0\n",
    "        self.writer = None\n",
    "        \n",
    "    def __call__(self, features, labels):\n",
    "        if self.counter == 0:    \n",
    "            current_path = self.filename + '0.tfrecord'\n",
    "            self.writer = tf.python_io.TFRecordWriter(current_path)\n",
    "        elif self.shard_size and self.counter % self.shard_size == 0:\n",
    "            self.shards += 1\n",
    "            if self.counter != 0:\n",
    "                self.writer.close()\n",
    "                shard_counter = self.counter/self.shard_size\n",
    "                current_path = self.filename + shard_counter + '.tfrecord'\n",
    "                self.writer = tf.python_io.TFRecordWriter(current_path)\n",
    "        \n",
    "        self.tf_convert(features, labels)\n",
    "        self.counter += 1\n",
    "\n",
    "    \n",
    "    def tf_convert(self, features, labels):\n",
    "        '''\n",
    "        Converts a single example into a tfrecord file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        features: dict of feature names and values\n",
    "        labels: dict of class labels as int\n",
    "        writer: a TFRecordWriter\n",
    "        '''\n",
    "        def wrap_int64(value):\n",
    "            # Wrapper class labels\n",
    "            return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "        def wrap_bytes(value):\n",
    "            # Wrapper for any feature data as byte type\n",
    "            return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "        def serialize(data):\n",
    "            # Wrap the data dict as TensorFlow Features.\n",
    "            # Wrap again as a TensorFlow Example.\n",
    "            # Serialize the data.\n",
    "            feature = tf.train.Features(feature=data)\n",
    "            example = tf.train.Example(features=feature)\n",
    "            return example.SerializeToString()\n",
    "\n",
    "        # Wrap features as bytes, labels as in64 and combine dictionaries\n",
    "        data = {**{k: wrap_int64(v) for k, v in features.items()}, \n",
    "                **{k: wrap_int64(v) for k, v in labels.items()}}\n",
    "\n",
    "        serialized = serialize(data)\n",
    "        self.writer.write(serialized)\n",
    "        \n",
    "    def close(self):\n",
    "        if self.writer:\n",
    "            self.writer.close()\n",
    "        self.writer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self, parse_fn, **kwargs):\n",
    "        self.parse = parse_fn\n",
    "        self.word2idx = kwargs['word2idx']\n",
    "        self.contextresponse = kwargs['contextresponse']\n",
    "\n",
    "    def __call__(self, string):\n",
    "        samples = self.parse(string, self.word2idx)\n",
    "        yield from self.contextresponse(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    return string.split()\n",
    "\n",
    "def parse(string, word2idx):\n",
    "    dialogue = json.loads(string)['chat']\n",
    "    samples = []\n",
    "    for utt in dialogue:\n",
    "        utt = utt.lower()\n",
    "        utt = tokenize(utt)\n",
    "        \n",
    "        # Convert tokens to embedding index\n",
    "        if any(utt):\n",
    "            for i, word in enumerate(utt):\n",
    "                if word not in word2idx:\n",
    "                    word2idx[word] =  len(word2idx)\n",
    "                utt[i] = word2idx[word]\n",
    "            samples.append(utt)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/2015-chats2_clean.json'\n",
    "mini_path = 'data/mini.json'\n",
    "w2v_path = 'data/GoogleNews-vectors-negative300.bin.gz'\n",
    "shard_path = 'data/training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: word2vec\n",
      "227.30807486277365\n"
     ]
    }
   ],
   "source": [
    "print('Loading: word2vec')\n",
    "tic = timer()\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "w2i = {k:i+1 for i,k in enumerate(w2v.index2word)}\n",
    "i2w = {v:k for k,v in w2i.items()}\n",
    "del w2v\n",
    "toc = timer()\n",
    "print(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = RecordStream(data_path)\n",
    "metrics = Metrics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records: 100 scanned, 0.03MB, 0.00sec\n"
     ]
    }
   ],
   "source": [
    "stream = RecordStream(mini_path)\n",
    "augment = ContextResponse(len(stream), context_size=3, buffer_size=1e2)\n",
    "parser = Parser(parse, word2idx=w2i, contextresponse=augment)\n",
    "tracker = ProgressTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0, 'response': [4, 3003532], 'context': [[260092], [1070, 16], [78493]]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "[4, 3003532] has type <class 'list'>, but expected one of: ((<class 'bytes'>,),)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-203d1306fc77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             writer({k:v for k,v in sample.items() if k in ['context', 'response']},\n\u001b[1;32m----> 6\u001b[1;33m                    {k:v for k,v in sample.items() if k in ['label']})\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mtracker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtracker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-87-e7bbd910ac8c>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, features, labels)\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-87-e7bbd910ac8c>\u001b[0m in \u001b[0;36mtf_convert\u001b[1;34m(self, features, labels)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# Wrap features as bytes, labels as in64 and combine dictionaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         data = {**{k: wrap_bytes(v) for k, v in features.items()}, \n\u001b[0m\u001b[0;32m     63\u001b[0m                 **{k: wrap_int64(v) for k, v in labels.items()}}\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-87-e7bbd910ac8c>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# Wrap features as bytes, labels as in64 and combine dictionaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         data = {**{k: wrap_bytes(v) for k, v in features.items()}, \n\u001b[0m\u001b[0;32m     63\u001b[0m                 **{k: wrap_int64(v) for k, v in labels.items()}}\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-87-e7bbd910ac8c>\u001b[0m in \u001b[0;36mwrap_bytes\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrap_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;31m# Wrapper for any feature data as byte type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    508\u001b[0m             field_value = [_GetIntegerEnumValue(field.enum_type, val)\n\u001b[0;32m    509\u001b[0m                            for val in field_value]\n\u001b[1;32m--> 510\u001b[1;33m           \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfield_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpp_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_FieldDescriptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCPPTYPE_MESSAGE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\u001b[0m in \u001b[0;36mextend\u001b[1;34m(self, elem_seq)\u001b[0m\n\u001b[0;32m    273\u001b[0m       \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m     \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_type_checker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCheckValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0melem_seq_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    273\u001b[0m       \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m     \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_type_checker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCheckValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0melem_seq_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\type_checkers.py\u001b[0m in \u001b[0;36mCheckValue\u001b[1;34m(self, proposed_value)\u001b[0m\n\u001b[0;32m    107\u001b[0m       message = ('%.1024r has type %s, but expected one of: %s' %\n\u001b[0;32m    108\u001b[0m                  (proposed_value, type(proposed_value), self._acceptable_types))\n\u001b[1;32m--> 109\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mproposed_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: [4, 3003532] has type <class 'list'>, but expected one of: ((<class 'bytes'>,),)"
     ]
    }
   ],
   "source": [
    "with closing(RecordWriter(shard_path, shard_size=None)) as writer:\n",
    "    for record in stream:\n",
    "        for sample in parser(record):\n",
    "            print(sample)\n",
    "            writer({k:v for k,v in sample.items() if k in ['context', 'response']},\n",
    "                   {k:v for k,v in sample.items() if k in ['label']})\n",
    "            tracker()\n",
    "tracker(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gg\n",
      "lol\n",
      "i\n",
      "ez\n",
      "you\n",
      "?\n",
      "u\n",
      "wp\n",
      "a\n",
      "is\n"
     ]
    }
   ],
   "source": [
    "for i in metrics.vocabulary.most_common(10):\n",
    "    print(i2w[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [[71462], [1590994], [200, 16]], 'response': [7727]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v for k,v in sample.items() if k in ['context', 'response']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [[260092], [1070, 16], [78493]],\n",
       " 'label': 0,\n",
       " 'response': [4, 3003532]}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records: 500032 scanned, 175.51MB, 0.67sec\n",
      "Progress: 500000 samples, 2.74sec [======================================>-]\r"
     ]
    }
   ],
   "source": [
    "s = RecordStream(data_path)\n",
    "t = ProgressTracker(total=len(s))\n",
    "for i in s:\n",
    "    t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
